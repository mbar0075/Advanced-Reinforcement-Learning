{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"-0.1\">\n",
    "\n",
    "**<center><h1>Exploring Reinforcement Learning Methods to approximate the Lunar Lander problem</h1></center>**\n",
    "**<center><h2>Advanced Reinforcement Learning Assignment (Actor Critic Experiment)</h2></center>**\n",
    "**<center><h3>Matthias Bartolo</h3></center>**\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>Package installation</h5>**\n",
    "\n",
    "<font size=\"-0.1\">\n",
    "\n",
    "```pip\n",
    "conda install swig\n",
    "conda install nomkl\n",
    "pip install gymnasium[all]\n",
    "pip install ufal.pybox2d\n",
    "pip install pygame\n",
    "pip install renderlab\n",
    "pip install numpy\n",
    "pip install matplotlib\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118/torch_stable.html\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>Package imports</h5>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available, device is set to cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import renderlab as rl\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Sequence\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setting the device to cpu as it was faster than gpu for this task\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available, device is set to {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>Declaring Hyperparameters</h5>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate α ∈ (0, 1] controls how much we update our current value estimates towards newly received returns.\n",
    "ALPHA = 0.0003\n",
    "# Gamma refers to the discount factor γ ∈ [0, 1]. It quantifies how much importance is given to future rewards.\n",
    "GAMMA = 0.99\n",
    "# The batch size is the number of training examples used in one iteration (that is, one gradient update) of training.\n",
    "BATCH_SIZE = 128\n",
    "# The buffer size is the number of transitions stored in the replay buffer, which the agent samples from to learn.\n",
    "BUFFER_SIZE = 10000\n",
    "# The minimum replay size is the minimum number of transitions that need to be stored in the replay buffer before the agent starts learning.\n",
    "MIN_REPLAY_SIZE = 5000\n",
    "# The maximum replay size is the maximum number of transitions that can be stored in the replay buffer.\n",
    "MAX_REPLAY_SIZE = 50\n",
    "# Episode start, episode end and episode decay are the parameters for the epsilon greedy policy.\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.997\n",
    "# The target update frequency is the frequency with which the target network is updated.\n",
    "TARGET_UPDATE_FREQ = 5\n",
    "# The success criteria is the average reward over the last 50 episodes that the agent must achieve to be considered successful.\n",
    "SUCCESS_CRITERIA = 195\n",
    "# The number of environments to run in parallel\n",
    "NUM_ENVS = 64\n",
    "# The number of neurons in the hidden layer\n",
    "HIDDEN_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>Environment Setup</h5>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.53057480e-03,  1.42091250e+00,  2.56304681e-01,\n",
       "          4.44108158e-01, -2.92551960e-03, -5.80568425e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.19755177e-03,  1.39875138e+00,  2.22572923e-01,\n",
       "         -5.40838003e-01, -2.53961608e-03, -5.04160933e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-1.60093303e-03,  1.40937567e+00, -1.62177622e-01,\n",
       "         -6.86453208e-02,  1.86193432e-03,  3.67356613e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-3.01208487e-03,  1.41277552e+00, -3.05111289e-01,\n",
       "          8.24583247e-02,  3.49707413e-03,  6.91122040e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 6.09970093e-03,  1.39992833e+00,  6.17814660e-01,\n",
       "         -4.88537729e-01, -7.06121139e-03, -1.39944300e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-3.57818615e-04,  1.41103745e+00, -3.62634584e-02,\n",
       "          5.21482015e-03,  4.21458215e-04,  8.21423065e-03,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-4.03881073e-04,  1.41829991e+00, -4.09160964e-02,\n",
       "          3.27996403e-01,  4.74705681e-04,  9.26809572e-03,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-4.36964026e-03,  1.40002728e+00, -4.42607164e-01,\n",
       "         -4.84132767e-01,  5.07007306e-03,  1.00257099e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-5.79900760e-03,  1.41893661e+00, -5.87398171e-01,\n",
       "          3.56276959e-01,  6.72646007e-03,  1.33054584e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.27088924e-03,  1.41696084e+00,  2.30006486e-01,\n",
       "          2.68470019e-01, -2.62466981e-03, -5.20998761e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-2.44045258e-03,  1.41687775e+00, -2.47207686e-01,\n",
       "          2.64785081e-01,  2.83466349e-03,  5.59961796e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-7.55434018e-03,  1.41594827e+00, -7.65203357e-01,\n",
       "          2.23442987e-01,  8.76052957e-03,  1.73329845e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-7.14931497e-03,  1.40389454e+00, -7.24173427e-01,\n",
       "         -3.12272012e-01,  8.29114020e-03,  1.64036021e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 5.64746838e-03,  1.40777767e+00,  5.72015405e-01,\n",
       "         -1.39676169e-01, -6.53723534e-03, -1.29569978e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 5.58757805e-04,  1.41308045e+00,  5.65852448e-02,\n",
       "          9.60129350e-02, -6.40718266e-04, -1.28174033e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.34508514e-03,  1.40482187e+00,  2.37524942e-01,\n",
       "         -2.71023870e-01, -2.71066139e-03, -5.38028665e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-6.30350132e-03,  1.40980780e+00, -6.38496280e-01,\n",
       "         -4.94537205e-02,  7.31102750e-03,  1.44628838e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.21402743e-04,  1.42109001e+00,  1.22693535e-02,\n",
       "          4.51991290e-01, -1.33757159e-04, -2.77920137e-03,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-1.96895609e-03,  1.42140770e+00, -1.99452311e-01,\n",
       "          4.66116577e-01,  2.28834269e-03,  4.51789424e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 5.69372158e-03,  1.41523671e+00,  5.76709330e-01,\n",
       "          1.91844165e-01, -6.59093587e-03, -1.30633265e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.53331761e-03,  1.40412760e+00,  1.55297190e-01,\n",
       "         -3.01886678e-01, -1.76996831e-03, -3.51770818e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 6.42566662e-03,  1.40979338e+00,  6.50836110e-01,\n",
       "         -5.00910990e-02, -7.43895536e-03, -1.47424042e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 5.37509937e-03,  1.40393031e+00,  5.44437647e-01,\n",
       "         -3.10674101e-01, -6.22175913e-03, -1.23323418e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-7.02371588e-03,  1.40286803e+00, -7.11444259e-01,\n",
       "         -3.57895344e-01,  8.14552419e-03,  1.61152631e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.31893161e-04,  1.40925562e+00,  1.33373681e-02,\n",
       "         -7.39765391e-02, -1.45996397e-04, -3.02113476e-03,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.11706152e-03,  1.40719199e+00,  2.14426994e-01,\n",
       "         -1.65693358e-01, -2.44643330e-03, -4.85708937e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 7.42712012e-03,  1.41114569e+00,  7.52262056e-01,\n",
       "          9.99761745e-03, -8.59930273e-03, -1.70398682e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.87227635e-03,  1.40275049e+00,  2.90920615e-01,\n",
       "         -3.63098115e-01, -3.32151703e-03, -6.58977777e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-4.80422983e-03,  1.41678274e+00, -4.86630261e-01,\n",
       "          2.60552615e-01,  5.57367597e-03,  1.10229075e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.65216827e-03,  1.40930891e+00,  2.68631846e-01,\n",
       "         -7.16172904e-02, -3.06650717e-03, -6.08491115e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 6.29749289e-03,  1.39991522e+00,  6.37850165e-01,\n",
       "         -4.89119977e-01, -7.29040522e-03, -1.44482553e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 5.04484167e-03,  1.40862918e+00,  5.10970473e-01,\n",
       "         -1.01828344e-01, -5.83889242e-03, -1.15742460e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.39954004e-03,  1.40227175e+00,  2.43034840e-01,\n",
       "         -3.84369940e-01, -2.77369912e-03, -5.50509021e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-2.55393970e-04,  1.41407585e+00, -2.58846395e-02,\n",
       "          1.40252367e-01,  3.02722619e-04,  5.86325629e-03,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-7.58552551e-03,  1.40748668e+00, -7.68348336e-01,\n",
       "         -1.52628988e-01,  8.79651029e-03,  1.74042374e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 3.19795613e-03,  1.41153908e+00,  3.23910445e-01,\n",
       "          2.75105406e-02, -3.69892991e-03, -7.33706132e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.24657054e-03,  1.41889417e+00,  2.27539986e-01,\n",
       "          3.54405046e-01, -2.59646121e-03, -5.15412092e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 4.70943470e-03,  1.39892876e+00,  4.76999462e-01,\n",
       "         -5.32960355e-01, -5.45026315e-03, -1.08047403e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 7.95249920e-03,  1.42028141e+00,  8.05483937e-01,\n",
       "          4.16038841e-01, -9.20813903e-03, -1.82454199e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.14128499e-03,  1.41072452e+00,  2.16875196e-01,\n",
       "         -8.69187899e-03, -2.47443933e-03, -4.91254851e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-3.94115457e-03,  1.40781772e+00, -3.99217457e-01,\n",
       "         -1.37897506e-01,  4.57364740e-03,  9.04285982e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 6.65388117e-03,  1.40242839e+00,  6.73942804e-01,\n",
       "         -3.77435386e-01, -7.70332199e-03, -1.52658075e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-3.89671326e-03,  1.41295576e+00, -3.94715577e-01,\n",
       "          9.04701278e-02,  4.52214666e-03,  8.94090235e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.71689992e-03,  1.42089105e+00,  1.73889890e-01,\n",
       "          4.43151832e-01, -1.98269356e-03, -3.93886939e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-1.33914943e-03,  1.40597761e+00, -1.35660946e-01,\n",
       "         -2.19672099e-01,  1.55856821e-03,  3.07291988e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 7.54899997e-03,  1.42178273e+00,  7.64617443e-01,\n",
       "          4.82762158e-01, -8.74061789e-03, -1.73197269e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 6.99148187e-03,  1.41475093e+00,  7.08149314e-01,\n",
       "          1.70246229e-01, -8.09463672e-03, -1.60406485e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-7.54718762e-03,  1.41276145e+00, -7.64474869e-01,\n",
       "          8.18085447e-02,  8.75220634e-03,  1.73164904e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-5.72729111e-03,  1.40924621e+00, -5.80128193e-01,\n",
       "         -7.44059235e-02,  6.64328272e-03,  1.31407663e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 6.63290033e-03,  1.40958595e+00,  6.71824336e-01,\n",
       "         -5.93191013e-02, -7.67906848e-03, -1.52178168e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-3.90892010e-03,  1.40997350e+00, -3.95952970e-01,\n",
       "         -4.20771949e-02,  4.53631952e-03,  8.96892324e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 4.29582596e-03,  1.41506851e+00,  4.35102284e-01,\n",
       "          1.84364468e-01, -4.97096544e-03, -9.85572487e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 8.35418701e-04,  1.40601337e+00,  8.46029595e-02,\n",
       "         -2.18070507e-01, -9.61255748e-04, -1.91638526e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-1.47333147e-03,  1.41563654e+00, -1.49253249e-01,\n",
       "          2.09623024e-01,  1.71406183e-03,  3.38081382e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 4.66537476e-03,  1.41353250e+00,  4.72535223e-01,\n",
       "          1.16099790e-01, -5.39920572e-03, -1.07036173e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-4.33711987e-03,  1.41802931e+00, -4.39318657e-01,\n",
       "          3.15952986e-01,  5.03243739e-03,  9.95123759e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 6.08587265e-03,  1.40352190e+00,  6.16413057e-01,\n",
       "         -3.28822047e-01, -7.04517169e-03, -1.39626801e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-6.33621239e-04,  1.40945292e+00, -6.41968846e-02,\n",
       "         -6.52106106e-02,  7.41015945e-04,  1.45415487e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 7.45592127e-03,  1.40838003e+00,  7.55176246e-01,\n",
       "         -1.12913005e-01, -8.63264594e-03, -1.71058699e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.14710226e-03,  1.39879537e+00,  2.17465729e-01,\n",
       "         -5.38875639e-01, -2.48120143e-03, -4.92592268e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-4.04834747e-03,  1.40618432e+00, -4.10056829e-01,\n",
       "         -2.10488230e-01,  4.69767489e-03,  9.28839147e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.33926764e-03,  1.40495920e+00,  2.36929506e-01,\n",
       "         -2.64918536e-01, -2.70384573e-03, -5.36679439e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.80721283e-03,  1.41763175e+00,  1.83035761e-01,\n",
       "          2.98287094e-01, -2.08733673e-03, -4.14603874e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 7.64427194e-03,  1.41373742e+00,  7.74268568e-01,\n",
       "          1.25187859e-01, -8.85101687e-03, -1.75383329e-01,\n",
       "          0.00000000e+00,  0.00000000e+00]], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the \"LunarLanderContinuous-v2\" environment from gymnasium with the render mode set to rgb_array and the number of environments set to NUM_ENVS\n",
    "env = gym.vector.make(\"LunarLanderContinuous-v2\", render_mode=\"rgb_array\", num_envs=NUM_ENVS, asynchronous=False)\n",
    "\n",
    "# Resetting the environment\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>Visualising Observation Space</h5>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space consists of a Box of shape (64, 8)\n",
      "\u001b[1mA single observation space consists of the following values:\u001b[0m\n",
      "\u001b[37mX position: \u001b[0m0.0026765824\n",
      "\u001b[36mY position: \u001b[0m1.4040436\n",
      "\u001b[35mX velocity: \u001b[0m0.27109635\n",
      "\u001b[34mY velocity: \u001b[0m-0.305629\n",
      "\u001b[33mAngle: \u001b[0m-0.003094715\n",
      "\u001b[32mAngular velocity: \u001b[0m-0.061407186\n",
      "\u001b[31mLeft leg touching the ground (0 or 1): \u001b[0m0.0\n",
      "\u001b[1mRight leg touching the ground (0 or 1): \u001b[0m0.0\n"
     ]
    }
   ],
   "source": [
    "# Checking the observation space\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(\"The observation space consists of a Box of shape {}\".format(obs.shape))\n",
    "\n",
    "# Setting observation space to the first observation\n",
    "obs = obs[0]\n",
    "print('\\033[1m' + \"A single observation space consists of the following values:\" + '\\033[0m')\n",
    "print('\\033[37m' + 'X position: ' + '\\033[0m' + obs[0].astype(str))\n",
    "print('\\033[36m' + 'Y position: ' + '\\033[0m' + obs[1].astype(str))\n",
    "print('\\033[35m' + 'X velocity: ' + '\\033[0m' + obs[2].astype(str))\n",
    "print('\\033[34m' + 'Y velocity: ' + '\\033[0m' + obs[3].astype(str))\n",
    "print('\\033[33m' + 'Angle: ' + '\\033[0m' + obs[4].astype(str))\n",
    "print('\\033[32m' + 'Angular velocity: ' + '\\033[0m' + obs[5].astype(str))\n",
    "print('\\033[31m' + 'Left leg touching the ground (0 or 1): ' + '\\033[0m' + obs[6].astype(str))\n",
    "print('\\033[1m' + 'Right leg touching the ground (0 or 1): ' + '\\033[0m' + obs[7].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>Visualising Action Space</h5>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space consists of a Box of shape (64, 2)\n",
      "\u001b[1mThe action space consists of the following values:\u001b[0m\n",
      "\u001b[35mValue 1 (Thrust): (-1.0, 1.0) for the main engine, whereby: \u001b[0m\n",
      "\u001b[35m[-1.0, -0.5] is off\u001b[0m\n",
      "\u001b[35m[0.5, 1.0] is on\u001b[0m\n",
      "\n",
      "\u001b[34mValue 2 (Rotation): (-1.0, 1.0) whereby:\u001b[0m\n",
      "\u001b[34m[-1.0, -0.5] is left\u001b[0m\n",
      "\u001b[34m[0.5, 1.0] is right\u001b[0m\n",
      "\u001b[34m[-0.5, 0.5] is off\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Checking the action space\n",
    "action_space = env.action_space\n",
    "\n",
    "print(\"The action space consists of a Box of shape {}\".format(action_space.shape))\n",
    "print('\\033[1m' + \"The action space consists of the following values:\" + '\\033[0m')\n",
    "print('\\033[35m' + 'Value 1 (Thrust): (-1.0, 1.0) for the main engine, whereby: ' + '\\033[0m')\n",
    "print('\\033[35m' + '[-1.0, -0.5] is off' + '\\033[0m')\n",
    "print('\\033[35m' + '[0.5, 1.0] is on' + '\\033[0m')\n",
    "print()\n",
    "print('\\033[34m' + 'Value 2 (Rotation): (-1.0, 1.0) whereby:' + '\\033[0m')\n",
    "print('\\033[34m' + '[-1.0, -0.5] is left' + '\\033[0m')\n",
    "print('\\033[34m' + '[0.5, 1.0] is right' + '\\033[0m')\n",
    "print('\\033[34m' + '[-0.5, 0.5] is off' + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>Actor Network</h5>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Actor network for the PPO algorithm\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The number of input features\n",
    "            hidden_units (int): The number of hidden units\n",
    "            output_size (int): The number of output features\n",
    "\n",
    "        Layers:\n",
    "            1. Linear layer with input_size and hidden_units\n",
    "            2. Tanh activation function\n",
    "            3. Linear layer with hidden_units and int(hidden_units/2)\n",
    "            4. Tanh activation function\n",
    "            5. Linear layer with int(hidden_units/2) and output_size\n",
    "            6. Linear layer with int(hidden_units/2) and output_size\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_units=64, output_size=2):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_units),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_units, int(hidden_units/2)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # The mu_head is the mean of the normal distribution\n",
    "        self.mu_head = nn.Linear(int(hidden_units/2),  output_size)\n",
    "\n",
    "        # The logstd_head is the standard deviation of the normal distribution\n",
    "        self.logstd_head = nn.Linear(int(hidden_units/2),  output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        # The mean is scaled by 2 and the standard deviation is exponentiated\n",
    "        loc = torch.tanh(self.mu_head(x)) * 2\n",
    "\n",
    "        # The standard deviation is exponentiated\n",
    "        scale = torch.exp(self.logstd_head(x))\n",
    "\n",
    "        # Returning the mean and standard deviation\n",
    "        return loc, scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>Critic Network</h5>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Critic network for the PPO algorithm\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The number of input features\n",
    "            hidden_units (int): The number of hidden units\n",
    "\n",
    "        Layers:\n",
    "            1. Linear layer with input_size and hidden_units\n",
    "            2. Tanh activation function\n",
    "            3. Linear layer with hidden_units and int(hidden_units/2)\n",
    "            4. Tanh activation function\n",
    "            5. Linear layer with int(hidden_units/2) and output_size\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_units=64):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_units),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_units, int(hidden_units/2)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # The value_head is the value of the state action pair\n",
    "        self.value_head = nn.Linear(int(hidden_units/2), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        value = self.value_head(x)\n",
    "        return value\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5>PPO Agent</h5>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent():\n",
    "    def __init__(self, env, running_memory, actor_net, critic_net, hidden_size=HIDDEN_SIZE, actor_lr=ALPHA, critic_lr=ALPHA, actor_optimizer=AdamW, critic_optimizer=AdamW, criterion=nn.SmoothL1Loss(), name=\"PPO\"):\n",
    "        self.env = env\n",
    "        self.running_memory = running_memory\n",
    "        self.ninputs = env.observation_space.shape[0]\n",
    "        self.noutputs = env.action_space.shape[0]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.criterion = criterion\n",
    "        self.actor_net = actor_net(self.ninputs, self.hidden_size, self.noutputs).to(device)\n",
    "        self.critic_net = critic_net(self.ninputs, self.hidden_size).to(device)\n",
    "\n",
    "        # Applying the initialize_weights function to the actor and critic networks\n",
    "        self.actor_net.apply(self.initialize_weights)\n",
    "        self.critic_net.apply(self.initialize_weights)\n",
    "\n",
    "        # Creating the optimizer for the actor and critic networks\n",
    "        self.actor_optimizer = actor_optimizer(self.actor_net.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = critic_optimizer(self.critic_net.parameters(), lr=self.critic_lr)\n",
    "\n",
    "        self.epsilon = EPS_START\n",
    "        self.steps_done = 0\n",
    "        self.episodes = 0\n",
    "        self.episode_info = {\"name\":name, \"episode_avg_rewards\": [], \"episode_lengths\": [], \"best_episode\": {\"episode\": 0, \"avg_reward\": np.NINF}, \"solved\": False, \"eps_duration\": 0}\n",
    "        self.display_every_n_episodes = 100\n",
    "        \n",
    "    def initialize_weights(self, layer):\n",
    "        \"\"\"\n",
    "            Initializes the weights of the layers of the actor and critic networks\n",
    "\n",
    "            Args:\n",
    "                layer (nn.Module): The layer to initialize the weights of\n",
    "        \"\"\"\n",
    "        # Checking if the layer is a linear layer\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            # Initializing the weights of the layer using the Xavier normal initialization\n",
    "            nn.init.xavier_normal_(layer.weight.data, nonlinearity='relu')\n",
    "\n",
    "            # Initializing the bias of the layer to 0 if it exists\n",
    "            if layer.bias is not None:\n",
    "                # Initializing the bias of the layer to 0\n",
    "                nn.init.constant_(layer.bias.data, 0)\n",
    "\n",
    "        # Checking if the layer is a batch normalization layer\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            # Initializing the weights of the layer to 1\n",
    "            nn.init.constant_(layer.weight.data, 1)\n",
    "\n",
    "            # Initializing the bias of the layer to 0\n",
    "            nn.init.constant_(layer.bias.data, 0)\n",
    "\n",
    "        # Checking if the layer is a linear layer\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            # Initializing the weights of the layer using the Xavier normal initialization\n",
    "            nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "            # Initializing the bias of the layer to 0 if it exists\n",
    "            nn.init.constant_(layer.bias.data, 0)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "            Saves the model\n",
    "\n",
    "            Args:\n",
    "                path (str): The path to save the model to\n",
    "        \"\"\"\n",
    "        # Creating the directory if it does not exist\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # Saving the actor and critic networks\n",
    "        torch.save(self.actor_net.state_dict(), path + \"/actor_net.pt\")\n",
    "        torch.save(self.critic_net.state_dict(), path + \"/critic_net.pt\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "            Loads the model\n",
    "\n",
    "            Args:\n",
    "                path (str): The path to load the model from\n",
    "        \"\"\"\n",
    "        # Loading the actor and critic networks\n",
    "        self.actor_net.load_state_dict(torch.load(path + \"/actor_net.pt\"))\n",
    "        self.critic_net.load_state_dict(torch.load(path + \"/critic_net.pt\"))\n",
    "\n",
    "    def get_episode_info(self):\n",
    "        \"\"\"Returns the episode info \"\"\"\n",
    "        return self.episode_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \"\"\"\n",
    "        The DQN agent that interacts with the environment\n",
    "\n",
    "        Args:\n",
    "            env: The environment to interact with\n",
    "            replay_buffer: The replay buffer to store and sample transitions from\n",
    "            target_update_freq: The frequency with which the target network is updated\n",
    "            criterion: The loss function used to train the policy network\n",
    "            name: The name of the agent (default: DQN)\n",
    "            network: The network used to estimate the action-value function (default: DQN)\n",
    "\n",
    "        Attributes:\n",
    "            env: The environment to interact with\n",
    "            replay_buffer: The replay buffer to store and sample transitions from\n",
    "            nsteps: The number of steps to run the agent for\n",
    "            target_update_freq: The frequency with which the target network is updated\n",
    "            ninputs: The number of inputs\n",
    "            noutputs: The number of outputs\n",
    "            policy_net: The policy network\n",
    "            target_net: The target network\n",
    "            optimizer: The optimizer used to update the policy network\n",
    "            criterion: The loss function used to train the policy network\n",
    "            epsilon: The probability of selecting a random action\n",
    "            steps_done: The number of steps the agent has run for\n",
    "            episodes: The number of episodes the agent has run for\n",
    "            episode_avg_rewards: The average reward for each episode\n",
    "            episode_lengths: The lengths of each episode\n",
    "            best_episode: The best episode\n",
    "            solved: Whether the environment is solved\n",
    "            display_every_n_episodes: The number of episodes after which the results are displayed\n",
    "            time: The time taken to run the agent\n",
    "    \"\"\"\n",
    "    def __init__(self, env, replay_buffer, target_update_freq, criterion=nn.SmoothL1Loss(), name=\"DQN\", network=DQN):\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.ninputs = env.observation_space.shape[0]\n",
    "        self.noutputs = env.action_space.n\n",
    "        self.policy_net = network(self.ninputs, self.noutputs).to(device)\n",
    "        self.target_net = network(self.ninputs, self.noutputs).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=ALPHA)\n",
    "        self.criterion = criterion\n",
    "        self.epsilon = EPS_START\n",
    "        self.steps_done = 0\n",
    "        self.episodes = 0\n",
    "        self.episode_info = {\"name\":name, \"episode_avg_rewards\": [], \"episode_lengths\": [], \"best_episode\": {\"episode\": 0, \"avg_reward\": np.NINF}, \"solved\": False, \"eps_duration\": 0}\n",
    "        self.display_every_n_episodes = 100\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Selects an action using an epsilon greedy policy \"\"\"\n",
    "        # Selecting a random action with probability epsilon\n",
    "        if random.random() <= self.epsilon: # Exploration\n",
    "            action = self.env.action_space.sample()\n",
    "        else: # Exploitation\n",
    "            # Selecting the action with the highest Q-value otherwise\n",
    "            with torch.no_grad():\n",
    "                state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "                qvalues = self.policy_net(state)\n",
    "                action = qvalues.argmax().item()\n",
    "        return action\n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\" Updates the policy network using a batch of transitions \"\"\"\n",
    "        # Sampling a batch of transitions from the replay buffer\n",
    "        states, actions, rewards, dones, next_states = self.replay_buffer.sample_batch()\n",
    "\n",
    "        # Converting the tensors to cuda tensors\n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        dones = dones.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        # Calculating the Q-values for the current states\n",
    "        qvalues = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Calculating the Q-values for the next states\n",
    "        with torch.no_grad():\n",
    "            # Calculating the Q-values for the next states using the target network (Q(s',a'))\n",
    "            target_qvalues = self.target_net(next_states)\n",
    "\n",
    "            # Calculating the maximum Q-values for the next states (max(Q(s',a'))\n",
    "            max_target_qvalues = torch.max(target_qvalues, axis=1).values.unsqueeze(1)\n",
    "\n",
    "            # Calculating the next Q-values using the Bellman equation (Q(s,a) = r + γ * max(Q(s',a')))\n",
    "            next_qvalues = rewards + GAMMA * (1 - dones.type(torch.float32)) * max_target_qvalues\n",
    "\n",
    "        # Calculating the loss\n",
    "        loss = self.criterion(qvalues, next_qvalues)\n",
    "\n",
    "        # Optimizing the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clipping the gradients\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Updating the target network\n",
    "        if self.episodes % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\" Updates epsilon \"\"\"\n",
    "        self.epsilon = max(EPS_END, EPS_DECAY * self.epsilon)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" Trains the agent for nsteps steps \"\"\"\n",
    "        # Resetting the environment\n",
    "        obs, _ = self.env.reset()\n",
    "\n",
    "        # Retrieving the starting time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Setting the episode_reward to 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Running the agent for nsteps steps\n",
    "        for step in itertools.count():\n",
    "            # Selecting an action\n",
    "            action = self.select_action(obs)\n",
    "\n",
    "            # Taking a step in the environment\n",
    "            new_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "            # Adding the reward to the cumulative reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Setting done to terminated or truncated\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Creating a transition\n",
    "            transition = Transition(obs, action, reward, done, new_obs)\n",
    "\n",
    "            # Appending the transition to the replay buffer\n",
    "            self.replay_buffer.append(transition)\n",
    "\n",
    "            # Resetting the observation\n",
    "            obs = new_obs\n",
    "\n",
    "            # Ending the episode and displaying the results if the episode is done\n",
    "            if done:\n",
    "                # Appending the rewards to the replay buffer\n",
    "                self.replay_buffer.rewards.append(episode_reward)\n",
    "\n",
    "                # Updating epsilon\n",
    "                self.update_epsilon()\n",
    "\n",
    "                # Resetting the environment\n",
    "                obs, _ = self.env.reset()\n",
    "\n",
    "                # Incrementing the number of episodes\n",
    "                self.episodes += 1\n",
    "\n",
    "                # Appending the average episode reward\n",
    "                self.episode_info[\"episode_avg_rewards\"].append(np.mean(self.replay_buffer.rewards))\n",
    "\n",
    "                # Appending the episode length\n",
    "                self.episode_info[\"episode_lengths\"].append(self.steps_done)\n",
    "\n",
    "                # Updating the best episode\n",
    "                if self.episode_info[\"episode_avg_rewards\"][-1] > self.episode_info[\"best_episode\"][\"avg_reward\"]:\n",
    "                    self.episode_info[\"best_episode\"][\"episode\"] = self.episodes\n",
    "                    self.episode_info[\"best_episode\"][\"avg_reward\"] = self.episode_info[\"episode_avg_rewards\"][-1]\n",
    "\n",
    "                # Checking if the environment is solved\n",
    "                if np.mean(self.episode_info[\"episode_avg_rewards\"][-MAX_REPLAY_SIZE:]) >= SUCCESS_CRITERIA:\n",
    "                    self.episode_info[\"solved\"] = True\n",
    "\n",
    "                # Checking if the environment is solved\n",
    "                if self.episode_info[\"solved\"]:\n",
    "                    print(\"\\033[32mSolved in {} episodes!\\033[0m\".format(self.episodes))\n",
    "                    print(\"-\" * 100)\n",
    "                    break\n",
    "                \n",
    "                # Displaying the results\n",
    "                if self.episodes % self.display_every_n_episodes == 0:\n",
    "                    print(\"\\033[35mEpisode:\\033[0m {} \\033[35mEpsilon:\\033[0m {:.2f} \\033[35mAverage Reward:\\033[0m {} \\033[35mEpisode Length:\\033[0m {}\".format(\n",
    "                                self.episodes,\n",
    "                                self.epsilon,\n",
    "                                self.episode_info[\"episode_avg_rewards\"][-1],\n",
    "                                self.episode_info[\"episode_lengths\"][-1])\n",
    "                        )\n",
    "                    print(\"-\" * 100)\n",
    "\n",
    "                # Resetting the cumulative reward\n",
    "                episode_reward = 0\n",
    "\n",
    "            # Updating the policy network\n",
    "            self.update()\n",
    "\n",
    "            # Updating the number of steps\n",
    "            self.steps_done += 1\n",
    "\n",
    "        # Retrieving the ending time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculating the time taken\n",
    "        self.episode_info[\"eps_duration\"] = end_time - start_time\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Runs the agent \"\"\"\n",
    "        # Initializing the replay buffer\n",
    "        self.replay_buffer.initialize()\n",
    "\n",
    "        # Training the agent\n",
    "        self.train()\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\" Tests the trained agent \"\"\"\n",
    "        # Resetting the environment\n",
    "        obs, _ = self.env.reset()\n",
    "\n",
    "        # Playing the environment\n",
    "        while True:\n",
    "            # Selecting the action with the highest Q-value\n",
    "            action = int(torch.argmax(self.policy_net(torch.from_numpy(obs).float().unsqueeze(0).to(device))).item())\n",
    "\n",
    "            # Taking a step in the environment\n",
    "            obs, _, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "            # Setting done to terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Playing the video\n",
    "        env.play()\n",
    "\n",
    "\n",
    "    def save(self, path=\"models/dqn\"):\n",
    "        \"\"\" Function to save the model \n",
    "            \n",
    "            Args:\n",
    "                path (str): The path to save the model to\n",
    "        \"\"\"\n",
    "        # Creating the directory if it does not exist\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # Saving the model\n",
    "        torch.save(self.policy_net.state_dict(), path + \"/policy_net.pth\")\n",
    "        torch.save(self.target_net.state_dict(), path + \"/target_net.pth\")\n",
    "\n",
    "        # Saving the episode info\n",
    "        np.save(path + \"/episode_info.npy\", self.episode_info)\n",
    "\n",
    "    def load(self, path=\"models/dqn\"):\n",
    "        \"\"\" Function to load the model \n",
    "            \n",
    "            Args:\n",
    "                path (str): The path to load the model from\n",
    "        \"\"\"\n",
    "        # Loading the model\n",
    "        self.policy_net.load_state_dict(torch.load(path + \"/policy_net.pth\"))\n",
    "        self.target_net.load_state_dict(torch.load(path + \"/target_net.pth\"))\n",
    "\n",
    "        # Loading the episode info\n",
    "        self.episode_info = np.load(path + \"/episode_info.npy\", allow_pickle=True).item()\n",
    "\n",
    "    def get_episode_info(self):\n",
    "        \"\"\" Returns the episode info \"\"\"\n",
    "        return self.episode_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
